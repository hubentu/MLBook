{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b930bc",
   "metadata": {},
   "source": [
    "# **Machine Learning for Time Series Data**\n",
    "\n",
    "Time series data is an ordered sequence of data points collected or recorded at specific time intervals. Examples include stock prices, weather data, sensor readings, sales data, and website traffic. Unlike traditional datasets, time series data has **temporal dependencies** that must be captured to make accurate predictions.\n",
    "\n",
    "In this section, we'll cover:\n",
    "\n",
    "1. **Types of Time Series Problems**\n",
    "2. **Key Challenges in Time Series Analysis**\n",
    "3. **Popular Machine Learning Algorithms for Time Series**\n",
    "4. **Deep Learning Approaches for Time Series**\n",
    "5. **Project: Forecasting Stock Prices**\n",
    "6. **Evaluation Metrics for Time Series**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Types of Time Series Problems**\n",
    "\n",
    "### **1.1. Time Series Forecasting**:\n",
    "The goal is to predict future values based on historical data. For example, predicting the temperature for the next 7 days, forecasting stock prices, or estimating future product demand.\n",
    "\n",
    "### **1.2. Time Series Classification**:\n",
    "Given a sequence of data, the goal is to classify the entire series or detect patterns within the series. For example, classifying an ECG signal as normal or abnormal or detecting fraudulent transactions.\n",
    "\n",
    "### **1.3. Anomaly Detection**:\n",
    "Identifying outliers or abnormal patterns in time series data. This is important in applications like network security (detecting unusual traffic), sensor monitoring (identifying faulty equipment), or financial systems (detecting fraudulent activities).\n",
    "\n",
    "### **1.4. Time Series Regression**:\n",
    "Predicting a continuous output where the input includes one or more time series variables. This is similar to regular regression but takes into account the time dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Key Challenges in Time Series Analysis**\n",
    "\n",
    "- **Temporal Dependency**: Time series data points are not independent of each other. Each point may be influenced by previous time steps, and capturing this dependency is crucial.\n",
    "- **Trend and Seasonality**: Time series data often exhibit trends (upward or downward movements) and seasonal patterns (regular fluctuations that repeat over time).\n",
    "- **Stationarity**: Many machine learning models assume that the data is stationary (its statistical properties do not change over time), which may not be the case for many real-world time series.\n",
    "- **Missing Data**: Time series data often contains missing values, which need to be handled carefully, as they can affect the model's predictions.\n",
    "- **Multivariate Time Series**: In some cases, multiple time series variables (e.g., temperature, humidity, wind speed) are correlated and need to be considered together.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Popular Machine Learning Algorithms for Time Series**\n",
    "\n",
    "Several traditional machine learning algorithms can be adapted for time series forecasting, classification, and regression. These include:\n",
    "\n",
    "### **3.1. ARIMA (AutoRegressive Integrated Moving Average)**\n",
    "- **ARIMA** is a widely used statistical method for time series forecasting. It combines three components: autoregression (AR), differencing to remove non-stationarity (I), and moving average (MA).\n",
    "  \n",
    "- **ARIMA** is suited for univariate time series data and is effective for short-term forecasting when the underlying data is stationary.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac4fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112.3353047  112.0833227  112.07379565 112.07343544 112.07342182]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Example univariate time series data (e.g., stock prices)\n",
    "data = [112, 118, 132, 129, 121, 135, 148, 148, 136, 119]\n",
    "\n",
    "# Fit an ARIMA model\n",
    "model = ARIMA(data, order=(1, 1, 1))  # (p, d, q) parameters\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast future values\n",
    "forecast = model_fit.forecast(steps=5)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012ff58",
   "metadata": {},
   "source": [
    "#### **3.2. Exponential Smoothing (ETS)**\n",
    "- **Exponential Smoothing** methods (e.g., Holt-Winters) are used for forecasting time series data with trends and seasonality. The model captures the level, trend, and seasonality of the data.\n",
    "  \n",
    "- It is particularly useful for making short-term forecasts.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d34fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[152.75000479 151.24999381 140.00000884 141.00000786 161.25000724]\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Example time series data with trend and seasonality\n",
    "data = [112, 118, 132, 129, 121, 135, 148, 148, 136, 119]\n",
    "\n",
    "# Fit an Exponential Smoothing model (Additive Trend and Seasonality)\n",
    "model = ExponentialSmoothing(data, trend='add', seasonal='add', seasonal_periods=4)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast future values\n",
    "forecast = model_fit.forecast(steps=5)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052b861",
   "metadata": {},
   "source": [
    "#### **3.3. Random Forest for Time Series**\n",
    "- **Random Forest** can be adapted to time series forecasting by using lagged values of the time series as features. For example, to predict \\(y_t\\), you might use \\(y_{t-1}\\), \\(y_{t-2}\\), etc., as input features.\n",
    "\n",
    "- **Random Forest** can capture non-linear relationships between lagged variables, making it more flexible than linear models like ARIMA.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868a9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138.82 133.41]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate time-lagged features\n",
    "data = pd.DataFrame({'value': [112, 118, 132, 129, 121, 135, 148, 148, 136, 119]})\n",
    "data['lag1'] = data['value'].shift(1)\n",
    "data['lag2'] = data['value'].shift(2)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Train a Random Forest model\n",
    "X = data[['lag1', 'lag2']]\n",
    "y = data['value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a43b9",
   "metadata": {},
   "source": [
    "#### **3.4. XGBoost for Time Series**\n",
    "- **XGBoost** is a gradient-boosting algorithm that can be adapted for time series forecasting. Similar to Random Forest, you create lagged features to capture the temporal dependencies in the data.\n",
    "\n",
    "- XGBoost is powerful for multivariate time series or datasets with complex relationships between variables.\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1181f8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 15.888949865206014\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Load and preprocess data\n",
    "# Assume data is a pandas DataFrame with a 'value' column and lagged features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an XGBoost model\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a06c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4. Deep Learning Approaches for Time Series**\n",
    "\n",
    "#### **4.1. Recurrent Neural Networks (RNNs)**\n",
    "- **RNNs** are designed to handle sequential data by passing the hidden state from one time step to the next. This makes them well-suited for time series forecasting and classification tasks where temporal dependencies are key.\n",
    "\n",
    "#### **4.2. Long Short-Term Memory (LSTM)**\n",
    "- **LSTMs** are a type of RNN designed to handle longer sequences by mitigating the vanishing gradient problem. They are widely used for time series forecasting when long-term dependencies are crucial.\n",
    "\n",
    "**Example: LSTM for Time Series Forecasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386a5091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 16:41:07.554190: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 16:41:07.562323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 16:41:07.572156: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 16:41:07.575107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 16:41:07.582663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 16:41:08.341890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729024868.843185  286816 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-15 16:41:08.873201: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119.23905]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import Input\n",
    "\n",
    "# Generate synthetic time series data\n",
    "data = np.array([112, 118, 132, 129, 121, 135, 148, 148, 136, 119])\n",
    "data = data.reshape((len(data), 1, 1))  # Reshape for LSTM input\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input((1, 1)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(data[:-1], data[1:], epochs=200, verbose=0)\n",
    "\n",
    "# Predict the next value\n",
    "x_input = np.array([119]).reshape((1, 1, 1))\n",
    "y_pred = model.predict(x_input)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430e8a3",
   "metadata": {},
   "source": [
    "#### **4.3. Convolutional Neural Networks (CNNs) for Time Series**\n",
    "- **CNNs** can be applied to time series by treating the data as 1D signals. By applying convolutional filters, CNNs can detect short-term patterns and features in time series data.\n",
    "\n",
    "#### **4.4. Transformer Models for Time Series**\n",
    "- **Transformers**, originally developed for NLP tasks, can also be used for time series forecasting. They rely on attention mechanisms to capture dependencies between different time steps, making them effective for multivariate time series with complex temporal relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Project: Forecasting Stock Prices Using LSTM**\n",
    "\n",
    "#### **Goal**:\n",
    "Predict future stock prices using LSTM based on historical prices.\n",
    "\n",
    "#### **Dataset**:\n",
    "Use the **Yahoo Finance API** to fetch historical stock prices for a company (e.g., Apple). You can use the `yfinance` library to easily access stock data.\n",
    "\n",
    "#### **Steps**:\n",
    "1. **Data Preprocessing**:\n",
    "   - Fetch historical stock prices (e.g., \"Close\" prices) using `yfinance`.\n",
    "   - Normalize the data\n",
    "\n",
    " using MinMax scaling.\n",
    "   - Create sequences of historical prices to be used as input for the LSTM model.\n",
    "\n",
    "2. **LSTM Model**:\n",
    "   - Build an LSTM model using **Keras** or **PyTorch**.\n",
    "   - Train the model to predict the next stock price based on the previous prices in the sequence.\n",
    "\n",
    "3. **Evaluation**:\n",
    "   - Evaluate the model using **Root Mean Squared Error (RMSE)** or **Mean Absolute Error (MAE)**.\n",
    "\n",
    "#### **Code Example: Stock Price Forecasting Using LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa106190",
   "metadata": {
    "tags": [
     "scroll-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 2s/step - loss: 0.8192"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6868"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.6054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1183"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0750"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0861"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0626"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0068"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0115"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0255"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0216"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0043"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0049"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0090"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m4/6\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0090"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0060"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0054"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0045"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0047"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0046"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 193ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[163.93512 164.47    165.09428 165.79855 166.51762 167.16321 167.7607\n",
      " 168.21623 168.51826 168.67763]\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import Input\n",
    "\n",
    "# Fetch historical stock prices (e.g., Apple)\n",
    "df = yf.download('AAPL', start='2023-01-01', end='2024-01-01')\n",
    "data = df['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data (create sequences of 60 days)\n",
    "X, y = [], []\n",
    "for i in range(60, len(data_scaled)):\n",
    "    X.append(data_scaled[i-60:i, 0])\n",
    "    y.append(data_scaled[i, 0])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)  # Reshape for LSTM\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input((X.shape[1], 1)))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict future stock prices\n",
    "y_pred = model.predict(X)\n",
    "y_pred_scaled = scaler.inverse_transform(y_pred)\n",
    "\n",
    "print(y_pred_scaled.reshape(-1)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305dbaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **6. Evaluation Metrics for Time Series**\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: Measures the average absolute difference between actual and predicted values.\n",
    "- **Root Mean Squared Error (RMSE)**: A more sensitive metric to large errors than MAE.\n",
    "- **Mean Absolute Percentage Error (MAPE)**: A percentage-based error metric useful for comparing across datasets with different scales.\n",
    "- **R-squared**: Measures the proportion of variance explained by the model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Time series analysis requires specialized models and methods due to its temporal structure. Traditional techniques like **ARIMA** and **Exponential Smoothing** are still effective for short-term univariate forecasting, while **machine learning models** like **Random Forest** and **XGBoost** are useful for more complex multivariate series. **Deep learning models** such as **LSTMs**, **CNNs**, and **Transformers** have become popular for capturing long-term dependencies and patterns in time series data."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "source_map": [
   10,
   63,
   76,
   84,
   97,
   105,
   127,
   135,
   152,
   166,
   189,
   224,
   268
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}