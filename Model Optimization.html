
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model Optimization &#8212; Machine Learning Notebook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Model Optimization';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Mixed Precision Training and Quantization" href="Mixed%20Precision%20and%20Quantization.html" />
    <link rel="prev" title="Core Machine Learning Concepts" href="Core%20ML%20concepts.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/mlb.png" class="logo__image only-light" alt="Machine Learning Notebook - Home"/>
    <script>document.write(`<img src="_static/mlb.png" class="logo__image only-dark" alt="Machine Learning Notebook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Machine Learning notebook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Core%20ML%20concepts.html">Core Machine Learning Concepts</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Model Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="Mixed%20Precision%20and%20Quantization.html">Mixed Precision Training and Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="system%20design.html">System Design for Machine Learning Models</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FModel Optimization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Model Optimization.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-based-optimization-techniques">Gradient-Based Optimization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-approach"><strong>Step-by-Step Approach:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-the-learning-rate"><strong>1. Reduce the Learning Rate</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-learning-rate-schedulers"><strong>2. Use Learning Rate Schedulers</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-1-and-2-parameters"><strong>3. Adjust β1 and β2 Parameters</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-gradient-clipping"><strong>4. Apply Gradient Clipping</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#increase-batch-size"><strong>5. Increase Batch Size</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-regularization"><strong>6. Add Regularization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-actions-to-stabilize-the-training-with-adam"><strong>Summary of Actions to Stabilize the Training with Adam:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-answer"><strong>Example Answer:</strong></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-optimization">
<h1>Model Optimization<a class="headerlink" href="#model-optimization" title="Link to this heading">#</a></h1>
<p>A crucial aspect of improving model performance and ensuring it generalizes well to unseen data. Optimization in machine learning can refer to several areas, such as hyperparameter tuning, gradient-based optimization techniques, and advanced strategies to improve model efficiency.</p>
<section id="hyperparameter-tuning">
<h2>Hyperparameter Tuning<a class="headerlink" href="#hyperparameter-tuning" title="Link to this heading">#</a></h2>
<p>Hyperparameters are parameters whose values are set before the learning process begins and are not learned from the data. Tuning these parameters effectively can significantly impact model performance.</p>
<p>Common Hyperparameters to Tune:</p>
<ul class="simple">
<li><p>Learning Rate: Controls the step size in gradient descent. Too high can cause the model to overshoot minima, too low can make convergence slow.
Batch Size: The number of samples processed before the model is updated. Smaller batches provide a more noisy estimate of the gradient but can lead to better generalization.</p></li>
<li><p>Number of Layers and Neurons (for DNNs): More layers and neurons can model more complex relationships but also increase the risk of overfitting.
Regularization Parameters: Such as L1/L2 penalties in regression models or dropout rate in neural networks.</p></li>
</ul>
<p>Tuning Methods:</p>
<ul class="simple">
<li><p>Grid Search: Tries every combination of hyperparameters in a specified range. It’s simple but computationally expensive.</p></li>
<li><p>Random Search: Samples hyperparameters randomly, often more efficient than grid search as it explores a wider range of values.</p></li>
<li><p>Bayesian Optimization: Builds a probabilistic model of the function mapping hyperparameters to the objective function (e.g., validation accuracy). It balances exploration and exploitation, making it more efficient than random or grid search.</p></li>
</ul>
<blockquote>
<div><p>Task 1:
Suppose you have trained a neural network for predicting house prices. What hyperparameters would you prioritize for tuning, and what approach would you take?</p>
</div></blockquote>
<p>Step-by-Step Approach:</p>
<ol class="arabic simple">
<li><p>Key Hyperparameters to Tune</p></li>
</ol>
<p>When tuning a neural network for a regression task like house price prediction, here are the most important hyperparameters to focus on:</p>
<ul class="simple">
<li><p>Learning Rate (LR):</p></li>
</ul>
<p>Why it’s important: The learning rate controls how much to change the model in response to the error at each step. Too high, and the model may overshoot; too low, and training will be slow.
How to tune: Start by testing a range of learning rates, often on a logarithmic scale (e.g., 0.1, 0.01, 0.001, etc.). You might also use a learning rate scheduler (which we’ll discuss later).</p>
<ul class="simple">
<li><p>Batch Size:</p></li>
</ul>
<p>Why it’s important: Batch size determines how many samples are processed before the model updates its weights. Smaller batches introduce more noise but may help with generalization. Larger batches make the gradient estimates more accurate but may cause overfitting.</p>
<p>How to tune: Common batch sizes are 32, 64, or 128, but it depends on the size of your dataset. Smaller batches often work better for large datasets.</p>
<ul class="simple">
<li><p>Number of Layers and Neurons:</p></li>
</ul>
<p>Why it’s important: More layers and neurons increase model complexity. However, too many layers may lead to overfitting, and too few may result in underfitting.</p>
<p>How to tune: Experiment with different architectures. You can try a simple 3-layer network and increase the number of layers and neurons gradually.</p>
<ul class="simple">
<li><p>Dropout Rate (Regularization):</p></li>
</ul>
<p>Why it’s important: Dropout is used to prevent overfitting by randomly turning off a fraction of neurons during each training step.</p>
<p>How to tune: Try dropout rates of 0.2, 0.3, and 0.5. Too high a dropout rate can slow learning and reduce the network’s capacity, while too low may not prevent overfitting effectively.</p>
<ul class="simple">
<li><p>Weight Initialization:</p></li>
</ul>
<p>Why it’s important: Improper weight initialization can cause slow convergence or even prevent the model from learning. Proper initialization ensures faster and more stable training.</p>
<p>How to tune: Use methods like <em>He initialization</em> for ReLU activations or <em>Xavier initialization</em> for sigmoid/tanh activations.</p>
<ol class="arabic simple" start="2">
<li><p>Choosing a Tuning Approach</p></li>
</ol>
<p>There are several approaches you can use to tune these hyperparameters:</p>
<ul class="simple">
<li><p>Grid Search:</p></li>
</ul>
<p>What it is: You systematically try every combination of hyperparameters within a predefined set.</p>
<p>Advantages: Guarantees that you explore all possibilities.</p>
<p>Disadvantages: Computationally expensive, especially with many hyperparameters.</p>
<ul class="simple">
<li><p>Random Search:</p></li>
</ul>
<p>What it is: Instead of trying all combinations, you randomly sample from the possible hyperparameter space.</p>
<p>Advantages: More efficient than grid search and often leads to good results.</p>
<p>Disadvantages: It doesn’t guarantee finding the best combination.</p>
<ul class="simple">
<li><p>Bayesian Optimization:</p></li>
</ul>
<p>What it is: This method builds a probabilistic model of the function mapping hyperparameters to model performance. It’s efficient because it balances exploring new areas of the hyperparameter space and exploiting known good areas.</p>
<p>Advantages: More efficient and smarter than random search and grid search.</p>
<p>Disadvantages: More complex to implement and requires a good understanding of the algorithm.</p>
<p>Example Answer:</p>
<p>To optimize the neural network for predicting house prices, I would prioritize tuning the learning rate, batch size, and number of layers and neurons. These hyperparameters have a significant impact on the convergence speed and generalization ability of the model.</p>
<p>First, I would start by performing random search to explore the space efficiently. I would define a reasonable range for each hyperparameter:</p>
<p>Learning Rate: [0.01, 0.001, 0.0001] <br />
Batch Size: [32, 64, 128] <br />
Number of Layers: [2, 3, 4] <br />
Number of Neurons per Layer: [64, 128, 256] <br />
Additionally, I would experiment with dropout rates to prevent overfitting, starting with values like [0.2, 0.3, 0.5].</p>
<p>After running the random search, I would analyze the results using cross-validation. Based on the best-performing combinations, I would refine the hyperparameter ranges and apply Bayesian optimization for more precise tuning, focusing on the most promising areas of the hyperparameter space.</p>
</section>
<section id="gradient-based-optimization-techniques">
<h2>Gradient-Based Optimization Techniques<a class="headerlink" href="#gradient-based-optimization-techniques" title="Link to this heading">#</a></h2>
<p>For models like neural networks, the training process involves minimizing a loss function using optimization algorithms. Understanding these algorithms and how to adjust their parameters is key to efficient training.</p>
<p>Common Optimization Algorithms:</p>
<ul class="simple">
<li><p>Stochastic Gradient Descent (SGD): Updates model parameters iteratively based on each training example. It’s computationally efficient but can be noisy.</p></li>
<li><p>Momentum: Accelerates SGD by adding a fraction of the previous update to the current update, which helps navigate flat regions in the loss surface.</p></li>
<li><p>Adam (Adaptive Moment Estimation): Combines the benefits of RMSprop (adaptive learning rate) and momentum. It’s popular for deep learning because it adapts the learning rate and smooths the updates.</p></li>
</ul>
<p>Key Parameters to Adjust:</p>
<ul class="simple">
<li><p>Learning Rate Schedule: Reducing the learning rate as training progresses can help the model converge to a better local minimum.</p></li>
<li><p>Weight Decay: A form of regularization that penalizes large weights to prevent overfitting.</p></li>
</ul>
<blockquote>
<div><p>Task:
You are using Adam as the optimizer for training a neural network. The model’s validation loss is fluctuating widely. How would you adjust the optimizer’s parameters or the learning process to stabilize the training?</p>
</div></blockquote>
<p>When the validation loss is fluctuating widely during training with the Adam optimizer, it typically suggests that the learning rate is either too high, or the model is experiencing some form of instability in its updates. Adam is generally a stable and efficient optimizer, but it can sometimes lead to oscillations in the validation loss due to its adaptive nature. Below is a step-by-step guide on how you can adjust the optimizer’s parameters and the learning process to stabilize the training.</p>
<section id="step-by-step-approach">
<h3><strong>Step-by-Step Approach:</strong><a class="headerlink" href="#step-by-step-approach" title="Link to this heading">#</a></h3>
</section>
<section id="reduce-the-learning-rate">
<h3><strong>1. Reduce the Learning Rate</strong><a class="headerlink" href="#reduce-the-learning-rate" title="Link to this heading">#</a></h3>
<p>A high learning rate is often the primary cause of instability and fluctuating loss values. Since Adam adapts the learning rate for each parameter, it might be that some parameters are being updated too aggressively.</p>
<ul class="simple">
<li><p><strong>Action</strong>: Start by reducing the learning rate.</p>
<ul>
<li><p>If the current learning rate is 0.001, you could reduce it to 0.0001 or 0.00001 and observe whether the validation loss stabilizes.</p></li>
</ul>
</li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Why this helps</strong>: Reducing the learning rate will allow the optimizer to take smaller, more controlled steps, which can prevent overshooting and stabilize the learning process.</p>
</section>
<section id="use-learning-rate-schedulers">
<h3><strong>2. Use Learning Rate Schedulers</strong><a class="headerlink" href="#use-learning-rate-schedulers" title="Link to this heading">#</a></h3>
<p>To ensure the learning rate is gradually reduced during training, you can implement a <strong>learning rate scheduler</strong>. This will dynamically decrease the learning rate as training progresses, especially if the validation loss starts to plateau.</p>
<ul class="simple">
<li><p><strong>Action</strong>: Use a <strong>Reduce On Plateau</strong> scheduler to reduce the learning rate when the validation loss stops improving.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ReduceLROnPlateau</span>

<span class="n">reduce_lr</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">reduce_lr</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Why this helps</strong>: This will automatically reduce the learning rate if no improvement in the validation loss is seen over several epochs, potentially reducing fluctuations.</p>
</section>
<section id="adjust-1-and-2-parameters">
<h3><strong>3. Adjust β1 and β2 Parameters</strong><a class="headerlink" href="#adjust-1-and-2-parameters" title="Link to this heading">#</a></h3>
<p>Adam uses two exponential decay rates for the moment estimates (β1 and β2), which are typically set to <strong>β1 = 0.9</strong> and <strong>β2 = 0.999</strong>. However, in cases where the model is unstable, these default values may not be ideal.</p>
<ul class="simple">
<li><p><strong>β1</strong>: Controls the momentum term, which smooths the gradient updates.</p></li>
<li><p><strong>β2</strong>: Controls how fast the squared gradient norm is updated.</p></li>
</ul>
<p>If the model is oscillating, it can help to increase β2 slightly to make the optimizer less sensitive to recent large gradients.</p>
<ul class="simple">
<li><p><strong>Action</strong>: Try increasing β2 (e.g., from 0.999 to 0.9995) to make the updates less aggressive.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta_2</span><span class="o">=</span><span class="mf">0.9995</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Why this helps</strong>: Increasing β2 will smooth the updates further, leading to more stable convergence.</p>
</section>
<section id="apply-gradient-clipping">
<h3><strong>4. Apply Gradient Clipping</strong><a class="headerlink" href="#apply-gradient-clipping" title="Link to this heading">#</a></h3>
<p>Fluctuations in validation loss can be caused by <strong>exploding gradients</strong>, particularly in deep neural networks or recurrent neural networks (RNNs). By applying gradient clipping, you can limit the size of the gradients to prevent extreme updates that destabilize training.</p>
<ul class="simple">
<li><p><strong>Action</strong>: Apply gradient clipping to limit the maximum value of the gradients during backpropagation. You can clip gradients to a specific threshold, such as 1.0.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">clipvalue</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Why this helps</strong>: Gradient clipping prevents the gradients from becoming too large, which can cause large updates and destabilize the learning process.</p>
</section>
<section id="increase-batch-size">
<h3><strong>5. Increase Batch Size</strong><a class="headerlink" href="#increase-batch-size" title="Link to this heading">#</a></h3>
<p>If the validation loss is fluctuating widely, it could also be due to noisy gradient updates from small batch sizes. Increasing the batch size can reduce the noise and make gradient estimates more accurate.</p>
<ul class="simple">
<li><p><strong>Action</strong>: Increase the batch size. If you’re using a batch size of 32, consider increasing it to 64 or 128.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Why this helps</strong>: Larger batch sizes produce more stable and accurate gradient updates by averaging out the noise over more data points.</p>
</section>
<section id="add-regularization">
<h3><strong>6. Add Regularization</strong><a class="headerlink" href="#add-regularization" title="Link to this heading">#</a></h3>
<p>If the model is overfitting, which can manifest as fluctuating validation loss, you may need to add regularization methods to stabilize the model. Common techniques include <strong>dropout</strong> or <strong>L2 regularization</strong> (also known as weight decay).</p>
<ul class="simple">
<li><p><strong>Action</strong>: Add dropout layers between the dense layers in your model, or add L2 regularization to your layers.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.regularizers</span> <span class="kn">import</span> <span class="n">l2</span>

<span class="c1"># Adding Dropout</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>  <span class="c1"># Dropout rate of 30%</span>

<span class="c1"># Adding L2 Regularization</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)))</span>
</pre></div>
</div>
<p><strong>Why this helps</strong>: Dropout prevents the model from relying too heavily on any particular neurons, and L2 regularization reduces the size of weights, helping to prevent overfitting.</p>
</section>
<hr class="docutils" />
<section id="summary-of-actions-to-stabilize-the-training-with-adam">
<h3><strong>Summary of Actions to Stabilize the Training with Adam:</strong><a class="headerlink" href="#summary-of-actions-to-stabilize-the-training-with-adam" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Reduce the learning rate</strong> to make gradient updates smaller and more controlled.</p></li>
<li><p><strong>Use a learning rate scheduler</strong>, such as Reduce On Plateau, to adaptively lower the learning rate when training plateaus.</p></li>
<li><p><strong>Adjust the β1 and β2 parameters</strong> of Adam to smooth updates and reduce sensitivity to large gradient updates.</p></li>
<li><p><strong>Apply gradient clipping</strong> to prevent gradients from becoming too large and causing instability.</p></li>
<li><p><strong>Increase the batch size</strong> to reduce noisy gradient updates.</p></li>
<li><p><strong>Add regularization</strong> like dropout or L2 regularization to prevent overfitting and improve stability.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="example-answer">
<h3><strong>Example Answer:</strong><a class="headerlink" href="#example-answer" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>When training my neural network using the Adam optimizer, I noticed that the validation loss fluctuates widely. To address this, I would start by <strong>reducing the learning rate</strong>. If the current learning rate is 0.001, I would decrease it to 0.0001 to ensure the updates are smaller and more stable.</p>
<p>Additionally, I would implement a <strong>Reduce On Plateau</strong> learning rate scheduler to automatically reduce the learning rate when the validation loss stops improving. This helps ensure that the model doesn’t overshoot local minima during training.</p>
<p>If the fluctuations persist, I would further adjust Adam’s <strong>β2 parameter</strong>, increasing it from 0.999 to 0.9995, to reduce sensitivity to recent large gradients. Finally, I would apply <strong>gradient clipping</strong> with a threshold of 1.0 to prevent the gradients from exploding, which can also cause fluctuations in the loss.</p>
<p>These adjustments should help stabilize the training process and reduce the variability in the validation loss.</p>
</div></blockquote>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Core%20ML%20concepts.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Core Machine Learning Concepts</p>
      </div>
    </a>
    <a class="right-next"
       href="Mixed%20Precision%20and%20Quantization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Mixed Precision Training and Quantization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-based-optimization-techniques">Gradient-Based Optimization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-approach"><strong>Step-by-Step Approach:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduce-the-learning-rate"><strong>1. Reduce the Learning Rate</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-learning-rate-schedulers"><strong>2. Use Learning Rate Schedulers</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-1-and-2-parameters"><strong>3. Adjust β1 and β2 Parameters</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-gradient-clipping"><strong>4. Apply Gradient Clipping</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#increase-batch-size"><strong>5. Increase Batch Size</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-regularization"><strong>6. Add Regularization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-actions-to-stabilize-the-training-with-adam"><strong>Summary of Actions to Stabilize the Training with Adam:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-answer"><strong>Example Answer:</strong></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Qiang Hu, AI assistant
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>